{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle \n",
    "import numpy as np\n",
    "import math\n",
    "import scipy\n",
    "\n",
    "import pandas as pd "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the pruned networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeltimestamp = '2022_11_05__01_04_10'\n",
    "\n",
    "dataOutput = '/gscratch/dynamicsai/otthomas/MothPruning/mothMachineLearning_dataAndFigs/DataOutput/Experiments/pruned_bias/pruned_bias/'\n",
    "modelSubdir = os.path.join(dataOutput, modeltimestamp)\n",
    "\n",
    "zscoreOutput = '/gscratch/dynamicsai/otthomas/MothMotifs/MothMotifs/DataOutput/zscoreTables/'\n",
    "zscoreSubdir = os.path.join(zscoreOutput, modeltimestamp)\n",
    "if not os.path.exists(zscoreSubdir):\n",
    "    os.mkdir(zscoreSubdir)\n",
    "\n",
    "preProcessSubdir = os.path.join(dataOutput, modeltimestamp, 'preprocessedMasks')\n",
    "if not os.path.exists(preProcessSubdir):\n",
    "    os.mkdir(preProcessSubdir)\n",
    "    \n",
    "#Load the networks\n",
    "sparseNetsFile = 'sparseNetworks.pkl'\n",
    "sparseNetworks = pickle.load(open(os.path.join( modelSubdir, sparseNetsFile), 'rb'))\n",
    "\n",
    "masksFile = 'masks_minmax_Adam5.pkl'\n",
    "masks = pickle.load(open(os.path.join(modelSubdir, masksFile), 'rb'))\n",
    "\n",
    "bmasksFile = 'bmasks_minmax_Adam5.pkl'\n",
    "bmasks = pickle.load(open(os.path.join(modelSubdir, bmasksFile), 'rb'))\n",
    "\n",
    "#Load the losses\n",
    "losses = pickle.load(open(os.path.join(modelSubdir, 'preprocessedNets', 'pruneLosses.pkl'),'rb'))\n",
    "\n",
    "losses = np.array(losses)\n",
    "losses = np.transpose(losses)\n",
    "\n",
    "lossesDF = pd.DataFrame(losses, columns=['0%', '15%', '25%', '35%', '45%', '55%', '65%', '75%', '85%', '90%', '91%', \n",
    "                                        '92%', '93%', '94%', '95%', '96%', '97%', '98%' ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "findSparseMasks = True\n",
    "\n",
    "if findSparseMasks == True:\n",
    "    #Only find all the sparse masks if they are not already saved\n",
    "    sparseMasks = []\n",
    "    for i in range(len(sparseNetworks)):\n",
    "        sparsity = sparseNetworks[i][0]\n",
    "        #Extract the masks using the sparsity index \n",
    "        # masks is organized by [sparsity index][layer number][network number]\n",
    "\n",
    "            \n",
    "        m = [masks[sparsity][j][i] for j in range(5)]\n",
    "        bm = [bmasks[sparsity][j][i] for j in range(5)]\n",
    "\n",
    "        #Combine mask and bias mask by adding bias mask as last row of mask \n",
    "        mask = [np.append(m[j], np.array(bm[j]).reshape([1, len(bm[j])]), axis=0) for j in range(5)]\n",
    "\n",
    "        sparseMasks.append((sparsity, mask))\n",
    "    \n",
    "    pickle.dump(sparseMasks, open(os.path.join(preProcessSubdir, 'sparseOptimalMasks.pkl'), 'wb'))\n",
    "\n",
    "else:\n",
    "    sparseMasks = pickle.load(open(os.path.join(preProcessSubdir, 'sparseOptimalMasks.pkl'), 'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Organization of masks for evaluating motifs throughout pruning.\n",
    "'''\n",
    "findallPrunedMasks = True\n",
    "\n",
    "if findallPrunedMasks == True:\n",
    "\n",
    "    #Only find all the sparse masks if they are not already saved\n",
    "    allPrunedNets = []\n",
    "    for i in range(len(sparseNetworks)):\n",
    "        net = []\n",
    "        for s in range(18): #18?\n",
    "            m = [masks[s][j][i] for j in range(5)] #combines all layers that belong to a network at a certain sparsity\n",
    "            bm = [bmasks[s][j][i] for j in range(5)]\n",
    "\n",
    "            mask = [np.append(m[j], np.array(bm[j]).reshape([1, len(bm[j])]), axis=0) for j in range(5)]\n",
    "            net.append((s, mask))\n",
    "\n",
    "        allPrunedNets.append(net) #should be len 400\n",
    "\n",
    "    print(len(allPrunedNets))\n",
    "    \n",
    "    pickle.dump(allPrunedNets, open(os.path.join(preProcessSubdir, 'allPrunedMasks.pkl'), 'wb'))\n",
    "\n",
    "else:\n",
    "    allPrunedNets = pickle.load(open(os.path.join(preProcessSubdir, 'allPrunedMasks.pkl'), 'rb'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove ghost nodes "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ghost nodes: nodes with no upstream input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmGhostNodes(masks, rm=True, allnets=False): \n",
    "    if rm == True:\n",
    "        sparseMasks_wo_ghosts = []\n",
    "\n",
    "        #We need slightly different code for removing ghost nodes from all pruned networks\n",
    "        if allnets == True:\n",
    "            for k in range(len(masks)): #iterate over the networks\n",
    "                net = []\n",
    "                for s in range(len(masks[k])): #iterate over the sparsities \n",
    "                    count = 0\n",
    "                    m = masks[k][s][1]\n",
    "                    #Iterate over the masking layers in each network \n",
    "                    for i in range(len(m)):\n",
    "                        #Iterate over the columns of the mask \n",
    "                        for j in range(len(m[i].T)):\n",
    "                            column = m[i].T[j]\n",
    "                            #Check to see if there are any connections between this node and the nodes in the previous layer. \n",
    "                            #If there are no connections, that means there are no upstream connections and this is a ghost node. \n",
    "                            n = np.count_nonzero(column)\n",
    "                            if n == 0:\n",
    "                                #print('Found a ghost node: %s node in layer %s.' % (j, i))\n",
    "                                count += 1\n",
    "                                #There is no input into this node \n",
    "                                #so make all downstream connections 0\n",
    "\n",
    "                                #i+1 gets us to the next mask \n",
    "                                #where the jth row is the ghost node \n",
    "                                m[i+1][j] = m[i+1][j] * 0\n",
    "                    net.append((masks[k][s][0],m))\n",
    "\n",
    "                sparseMasks_wo_ghosts.append(net)\n",
    "            \n",
    "        else:\n",
    "            for k in range(len(masks)):\n",
    "                count = 0\n",
    "                m = masks[k][1]\n",
    "                #Iterate over the masking layers in each network \n",
    "                for i in range(len(m)):\n",
    "                    #Iterate over the columns of the mask \n",
    "                    for j in range(len(m[i].T)):\n",
    "                        column = m[i].T[j]\n",
    "                        #Check to see if there are any connections between this node and the nodes in the previous layer. \n",
    "                        #If there are no connections, that means there are no upstream connections and this is a ghost node. \n",
    "                        n = np.count_nonzero(column)\n",
    "                        if n == 0:\n",
    "                            #print('Found a ghost node: %s node in layer %s.' % (j, i))\n",
    "                            count += 1\n",
    "                            #There is no input into this node \n",
    "                            #so make all downstream connections 0\n",
    "\n",
    "                            #i+1 gets us to the next mask \n",
    "                            #where the jth row is the ghost node \n",
    "                            m[i+1][j] = m[i+1][j] * 0\n",
    "\n",
    "                sparseMasks_wo_ghosts.append((masks[k][0],m))\n",
    "\n",
    "            pickle.dump(sparseMasks_wo_ghosts, open(os.path.join(preProcessSubdir, 'masks_wo_ghost_nodes.pkl'), 'wb'))\n",
    "\n",
    "    else:\n",
    "        sparseMasks_wo_ghosts = pickle.load(open(os.path.join(preProcessSubdir, 'masks_wo_ghost_nodes.pkl'), 'rb'))\n",
    "\n",
    "    return sparseMasks_wo_ghosts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove dead nodes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dead nodes: nodes with no downstream output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmDeadNodes(masks, rm=True, allNets=False): \n",
    "    if rm == True:\n",
    "        sparseMasks_wo_dead = []\n",
    "\n",
    "        #We need slightly different code for removing ghost nodes from all pruned networks\n",
    "        if allNets == True:\n",
    "            for k in range(len(masks)): #iterate over the networks\n",
    "                net = []\n",
    "                for s in range(len(masks[k])): #iterate over the sparsities \n",
    "                    count = 0\n",
    "                    m = masks[k][s][1]\n",
    "                    #Reverse iterate over the masking layers in each network, excluding the input and output layers\n",
    "                    for i in reversed(range(1, len(m))):\n",
    "                        #Iterate over the rows of the mask, skipping the bias (last row)\n",
    "                        for j in range(len(m[i])-1):\n",
    "                            row = m[i][j]\n",
    "                            #Check to see if there are any connections between this node and the nodes in the next layer. \n",
    "                            #If there are no connections, that means there are no downstream connections and this is a dead node. \n",
    "                            n = np.count_nonzero(row)\n",
    "                            if n == 0:\n",
    "                                #print('Found a dead node: %s node in layer %s.' % (j, i))\n",
    "                                count += 1\n",
    "                                #There is no output from this node \n",
    "                                #so make all upstream connections 0\n",
    "\n",
    "                                #i-1 gets us to the previous mask \n",
    "                                #where the jth column is the ghost node \n",
    "                                m[i-1].T[j] = m[i-1].T[j] * 0\n",
    "\n",
    "                    net.append((masks[k][s][0],m))\n",
    "                    \n",
    "                sparseMasks_wo_dead.append(net)\n",
    "        else:\n",
    "            for k in range(len(masks)):\n",
    "                count = 0\n",
    "                m = masks[k][1]\n",
    "                #Reverse iterate over the masking layers in each network, excluding the input and output layers\n",
    "                for i in reversed(range(1, len(m))):\n",
    "                    #Iterate over the rows of the mask, skipping the bias (last row)\n",
    "                    for j in range(len(m[i])-1):\n",
    "                        row = m[i][j]\n",
    "                        #Check to see if there are any connections between this node and the nodes in the next layer. \n",
    "                        #If there are no connections, that means there are no downstream connections and this is a dead node. \n",
    "                        n = np.count_nonzero(row)\n",
    "                        if n == 0:\n",
    "                            #print('Found a dead node: %s node in layer %s.' % (j, i))\n",
    "                            count += 1\n",
    "                            #There is no output from this node \n",
    "                            #so make all upstream connections 0\n",
    "\n",
    "                            #i-1 gets us to the previous mask \n",
    "                            #where the jth column is the ghost node \n",
    "                            m[i-1].T[j] = m[i-1].T[j] * 0\n",
    "\n",
    "                sparseMasks_wo_dead.append((masks[k][0],m))\n",
    "\n",
    "        pickle.dump(sparseMasks_wo_dead, open(os.path.join(preProcessSubdir, 'masks_wo_dead_nodes.pkl'), 'wb'))\n",
    "\n",
    "    else:\n",
    "        sparseMasks_wo_dead = pickle.load(open(os.path.join(preProcessSubdir, 'masks_wo_dead_nodes.pkl'), 'rb'))\n",
    "\n",
    "    return sparseMasks_wo_dead"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the motif z-score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motif counting functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-order motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fom(m):\n",
    "        '''\n",
    "        Calculates the number of first-order motifs in the network (equivalent to the number of edges).\n",
    "        \n",
    "        Input(s): the mask of the pruned network, as a list of matrices\n",
    "        Returns: FOM (total number of first-order motifs), FOMList (Number of weights and number of bias connections\n",
    "                in each layer)\n",
    "        '''\n",
    "        FOM = 0\n",
    "        FOMList = [[0,0],[0,0],[0,0],[0,0],[0,0]] #[[Num weights, num biases]] in each layer\n",
    "\n",
    "        for i in range(len(m)): \n",
    "                #Count number of connections between weights\n",
    "                w_connections = np.count_nonzero(m[i][0:-1])\n",
    "                FOMList[i][0] = w_connections\n",
    "                #Count number of connections from bias\n",
    "                b_connections = np.count_nonzero(m[i][-1])\n",
    "                FOMList[i][1] = b_connections\n",
    "\n",
    "                connections = w_connections + b_connections\n",
    "                FOM += connections\n",
    "        return FOM, FOMList"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second-order motifs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diverging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sodm(m):\n",
    "    '''\n",
    "    Calculates the number of second-order diverging motifs in the network.\n",
    "    Also calculates the remaining number of nodes in the network. \n",
    "        \n",
    "    Input(s): the mask of the pruned network, as a list of matrices\n",
    "    Returns: SODM (total number of second-order diverging motifs in the network), numFC (Number of remaining nodes \n",
    "        with downstream output)\n",
    "    '''\n",
    "    \n",
    "    SODM = 0\n",
    "    numFC = [0,0,0,0,0,0] #Number of remaining nodes with downstream output\n",
    "\n",
    "    for i in range(len(m)): \n",
    "        nodes = 0\n",
    "        #Calculate second-order diverging motifs\n",
    "        for row in m[i]:\n",
    "            n = np.count_nonzero(row)\n",
    "            if n >= 2:\n",
    "                SODM += math.factorial(n)/(math.factorial(2)*math.factorial(n-2))\n",
    "                    \n",
    "            #also calculate number of remaining nodes\n",
    "            if n > 0: \n",
    "                nodes += 1\n",
    "                    \n",
    "        numFC[i] = nodes\n",
    "            \n",
    "            #Count number of nodes in final layer \n",
    "        if i == 4: \n",
    "            nodes = 0 \n",
    "            for row in m[i].T:\n",
    "                n = np.count_nonzero(row)\n",
    "                if n > 0 :\n",
    "                    nodes += 1\n",
    "            numFC[i+1] = nodes\n",
    "    \n",
    "    return SODM, numFC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def socm(m):\n",
    "    '''\n",
    "    Calculates the number of second-order converging motifs in the network.\n",
    "        \n",
    "    Input(s): the mask of the pruned network, as a list of matrices\n",
    "    Returns: SOCM (total number of second-order converging motifs in the network)\n",
    "    '''\n",
    "\n",
    "    SOCM = 0\n",
    "    numFCUS = [0,0,0,0,0,0]\n",
    "\n",
    "    for i in range(len(m)):\n",
    "        nodes = 0\n",
    "        #Calculate second-order converging motifs\n",
    "        for column in m[i].T:\n",
    "            n = np.count_nonzero(column)\n",
    "            if n >= 2:\n",
    "                SOCM += math.factorial(n)/(math.factorial(2)*math.factorial(n-2))\n",
    "\n",
    "            \n",
    "            #also calculate number of remaining nodes with upstream input, skip input \n",
    "            if n > 0: \n",
    "                nodes += 1\n",
    "                    \n",
    "        numFCUS[i+1] = nodes\n",
    "\n",
    "    return SOCM, numFCUS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sochain(m):\n",
    "    '''\n",
    "    Calculates the number of second-order chain motifs in the network.\n",
    "        \n",
    "    Input(s): the mask of the pruned network, as a list of matrices\n",
    "    Returns: SOChain (total number of second-order chain motifs in the network)\n",
    "    ''' \n",
    "    \n",
    "    SOChain = 0 \n",
    "    for i in range(len(m)): \n",
    "        #Calculate second-order chain motifs\n",
    "        if i != 4: \n",
    "            #Exclude the bias by excluding the last row\n",
    "            SOChain += np.count_nonzero(np.matmul(m[i][0:-1],m[i+1][0:-1]))\n",
    "            \n",
    "            #Add in the motifs from the bias terms \n",
    "            SOChain += np.count_nonzero(np.matmul(m[i][-1],m[i+1][0:-1]))\n",
    "        else: \n",
    "            pass\n",
    "\n",
    "    return SOChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third-order motifs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diverging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def todm(m):\n",
    "    '''\n",
    "    Calculates the number of third-order diverging motifs in the network.\n",
    "        \n",
    "    Input(s): the mask of the pruned network, as a list of matrices\n",
    "    Returns: TODM (total number of third-order diverging motifs in the network)\n",
    "    '''\n",
    "\n",
    "    TODM = 0 \n",
    "    for i in range(len(m)): \n",
    "        #Calculate third-order diverging motifs\n",
    "        for row in m[i]:\n",
    "            n = np.count_nonzero(row)\n",
    "            if n >= 3:\n",
    "                TODM += math.factorial(n)/(math.factorial(3)*math.factorial(n-3))\n",
    "\n",
    "    return TODM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tocm(m):\n",
    "    '''\n",
    "    Calculates the number of third-order converging motifs in the network.\n",
    "        \n",
    "    Input(s): the mask of the pruned network, as a list of matrices\n",
    "    Returns: TOCM (total number of third-order converging motifs in the network)\n",
    "    '''\n",
    "\n",
    "    TOCM = 0 \n",
    "    for i in range(len(m)): \n",
    "        #Calculate third-order converging motifs \n",
    "        for column in m[i].T:\n",
    "            n = np.count_nonzero(column)\n",
    "            if n >= 3:\n",
    "                TOCM += math.factorial(n)/(math.factorial(2)*math.factorial(n-2))\n",
    "\n",
    "    return TOCM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tochain(m):\n",
    "    '''\n",
    "    Calculates the number of third-order chain motifs in the network.\n",
    "        \n",
    "    Input(s): the mask of the pruned network, as a list of matrices\n",
    "    Returns: TOChain (total number of third-order chain motifs in the network)\n",
    "    '''\n",
    "\n",
    "    TOChain = 0\n",
    "    for i in range(len(m)): \n",
    "        #Calculate third-order chain motifs \n",
    "        if i in (0,1,2): \n",
    "            #Count non-zero elements of (Layer 1 * Layer 2 * Layer 3)\n",
    "            #Exclude the bias by excluding the last row\n",
    "            m1 = np.matmul(m[i][0:-1],m[i+1][0:-1])\n",
    "            TOChain += np.count_nonzero(np.matmul(m1,m[i+2][0:-1]))\n",
    "                \n",
    "            #Add in the motifs from the bias terms \n",
    "            mbias = np.matmul(m[i][-1],m[i+1][0:-1])\n",
    "            TOChain += np.count_nonzero(np.matmul(mbias,m[i+2][0:-1]))\n",
    "        else: \n",
    "            pass\n",
    "\n",
    "    return TOChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bi-fan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bifan(m):\n",
    "    '''\n",
    "    Calculates the number of bi-fan motifs in the network.\n",
    "        \n",
    "    Input(s): the mask of the pruned network, as a list of matrices\n",
    "    Returns: BIFAN (total number of bi-fan motifs in the network)\n",
    "    '''\n",
    "    \n",
    "    BIFAN = 0\n",
    "\n",
    "    for i in range(len(m)): \n",
    "        #For each row, calculate the dot product of the row with the rest of the rows in the mask \n",
    "        for j in range(len(m[i])-1):\n",
    "            row = m[i][j]\n",
    "            mat = m[i][j+1:]\n",
    "            count = np.dot(mat, row) #Each element in count represents the number of bifans row j shares with all subsequent rows \n",
    "\n",
    "            #Calculate the number of bifans\n",
    "            for n in count: \n",
    "                n = int(n)\n",
    "                if n >= 2: \n",
    "                    BIFAN += math.factorial(n)/(math.factorial(2)*math.factorial(n-2))\n",
    "    \n",
    "    return BIFAN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bi-parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bipar(m):\n",
    "    '''\n",
    "    Calculates the number of bi-parallel motifs in the network.\n",
    "        \n",
    "    Input(s): the mask of the pruned network, as a list of matrices\n",
    "    Returns: BIPAR (total number of bi-parallel motifs in the network)\n",
    "    '''\n",
    "    BIPAR = 0 \n",
    "    for i in range(len(m)): \n",
    "        if i != 4: \n",
    "            #Find the product between two layers.\n",
    "            #Exclude the bias by excluding the last row\n",
    "            prod = np.matmul(m[i],m[i+1][0:-1])\n",
    "            \n",
    "            #Take the factorial of the whole product matrix. \n",
    "            #Factorial will have NaN values so np.sum may cause an error \n",
    "            fact_mat = scipy.special.factorial(prod)\n",
    "            \n",
    "            fact_2 = math.factorial(2)\n",
    "\n",
    "            fact_mat_2 = scipy.special.factorial(prod-2)\n",
    "\n",
    "            denom = fact_2 * fact_mat_2\n",
    "\n",
    "            comb_mat = np.divide(fact_mat, denom)\n",
    "\n",
    "            #Number of bi-parallel motifs is the sum of the resultant matrix\n",
    "            #np.nansum returns sum, treating NaN values as zero.\n",
    "            BIPAR += np.sum(np.ma.masked_invalid(comb_mat))\n",
    "            \n",
    "        else: \n",
    "            pass\n",
    "    \n",
    "    return BIPAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build random network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomPruning(FOMList, numFC): \n",
    "    '''\n",
    "    Randomly prunes a fully-connected random network to the same number of weights and nodes (in each layer respectively) as the real network. \n",
    "    This part of constructing the random network occurs before the ghost and dead nodal pruning of the real network.\n",
    "    '''\n",
    "\n",
    "    #Remove the bias from numFC\n",
    "    numFC = [numFC[i]-1 if FOMList[i][1] != 0 else numFC[i] for i in range(len(numFC)-1)] #range(len(numFC)-1) because there is never a bias in the output layer\n",
    "    numFC.append(7) #add in final layer\n",
    "\n",
    "    #Build fully-connected network of zeros with only the live nodes found after post-pruning\n",
    "    # the ineffuctual nodes. \n",
    "    r1 = np.zeros((numFC[0]*numFC[1]))\n",
    "    r2 = np.zeros((numFC[1]*numFC[2]))\n",
    "    r3 = np.zeros((numFC[2]*numFC[3]))\n",
    "    r4 = np.zeros((numFC[3]*numFC[4]))\n",
    "    r5 = np.zeros((numFC[4]*numFC[5]))\n",
    "\n",
    "    #Nlw corresponds to the number of remaining weights or biases in the real network. \n",
    "    Nlw1 = FOMList[0][0]\n",
    "    Nlw2 = FOMList[1][0]\n",
    "    Nlw3 = FOMList[2][0]\n",
    "    Nlw4 = FOMList[3][0]\n",
    "    Nlw5 = FOMList[4][0]\n",
    "\n",
    "    #Nln corresponds to the total number of live nodes after post-pruning associated with\n",
    "    # one layer (inc. input and output nodes). \n",
    "    Nln1 = numFC[0]+numFC[1]\n",
    "    Nln2 = numFC[1]+numFC[2]\n",
    "    Nln3 = numFC[2]+numFC[3]\n",
    "    Nln4 = numFC[3]+numFC[4]\n",
    "    Nln5 = numFC[4]+numFC[5]\n",
    "\n",
    "    #Nr corresponds to the difference between the total remaining weights and total remaining\n",
    "    # nodes. \n",
    "    Nr1 = Nlw1 - Nln1\n",
    "    Nr2 = Nlw2 - Nln2\n",
    "    Nr3 = Nlw3 - Nln3\n",
    "    Nr4 = Nlw4 - Nln4\n",
    "    Nr5 = Nlw5 - Nln5\n",
    "\n",
    "    NList = [(Nlw1, Nln1, Nr1),(Nlw2, Nln2, Nr2),(Nlw3, Nln3, Nr3),\n",
    "             (Nlw4, Nln4, Nr4),(Nlw5, Nln5, Nr5)]\n",
    "    \n",
    "    rList = [r1, r2, r3, r4, r5]\n",
    "\n",
    "    #print('Nlw1: %s, Nln1: %s, Nr1: %s' %(Nlw1, Nln1, Nr1))\n",
    "    #print('Nlw2: %s, Nln2: %s, Nr2: %s' %(Nlw2, Nln2, Nr2))\n",
    "    #print('Nlw3: %s, Nln3: %s, Nr3: %s' %(Nlw3, Nln3, Nr3))\n",
    "    #print('Nlw4: %s, Nln4: %s, Nr4: %s' %(Nlw4, Nln4, Nr4))\n",
    "    #print('Nlw5: %s, Nln5: %s, Nr5: %s' %(Nlw5, Nln5, Nr5))\n",
    "\n",
    "    #Set the first Nr values of the list to one if Nr is greater than zero and then randomly shuffle. \n",
    "    for i in range(len(rList)):\n",
    "        if NList[i][2] > 0:\n",
    "            rList[i][0:NList[i][2]] = 1\n",
    "            np.random.shuffle(rList[i])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "    #Reshape the matrices\n",
    "    for i in range(len(rList)):\n",
    "        rList[i] = np.reshape(rList[i], (numFC[i],numFC[i+1]))\n",
    "\n",
    "\n",
    "    #The following is to assure we have at least one live weight connected to each node in\n",
    "    # the layer (inputs and outputs). \n",
    "\n",
    "    for r in range(len(rList)):\n",
    "        if NList[r][2] > 0: \n",
    "            extraNodeCount = 0\n",
    "            #For each row place a random 1\n",
    "            for i in range(len(rList[r])):\n",
    "                #print('r: %s and i: %s' %(r,i))\n",
    "                row = np.array(rList[r][i])\n",
    "                zeroElements = np.nonzero(row==0)[0] #Finds all indices with a zero\n",
    "\n",
    "                #There is a small chance that there are no zero elements even after random shuffling. \n",
    "                #In this situation, we need to keep track of the extra node, and add it in later\n",
    "                if len(zeroElements) != 0: #if we've found a nonzero element\n",
    "                    idx = np.random.choice(zeroElements) #Picks a random index\n",
    "                    rList[r][i][idx] = 1 #Sets the value at that index to one\n",
    "                else: \n",
    "                    extraNodeCount += 1\n",
    "\n",
    "            #For each column place a random 1\n",
    "            for j in range(len(rList[r].T)):\n",
    "                #print('r: %s and j: %s' %(r,j))\n",
    "                col = np.array(rList[r].T[j])\n",
    "                zeroElements = np.nonzero(col==0)[0] #Finds all indices with a zero\n",
    "                if len(zeroElements) != 0:\n",
    "                    idx = np.random.choice(zeroElements) #Picks a random index\n",
    "                    rList[r].T[j][idx] = 1 #Sets the value at that index to one\n",
    "                else: \n",
    "                    extraNodeCount += 1\n",
    "\n",
    "            if extraNodeCount != 0: \n",
    "                #If we have some extra nodes, add them in randomly. \n",
    "                for n in range(extraNodeCount):\n",
    "                    z = np.nonzero(rList[r]==0)\n",
    "                    idx = np.random.choice(np.arange(len(z[0])))\n",
    "                    x = z[0][idx]\n",
    "                    y = z[1][idx]\n",
    "                    rList[r][x][y] = 1\n",
    "\n",
    "        else:\n",
    "            if r == 3:\n",
    "                if NList[r][2] < 0:\n",
    "                    if abs(NList[r][2]) <= (NList[r][0]-numFC[r]):\n",
    "                        flatr = rList[r].flatten()\n",
    "                        flatr[0:abs(NList[r][2])] = 1\n",
    "                        np.random.shuffle(flatr)\n",
    "                        rList[r] = np.reshape(flatr, (numFC[r],numFC[r+1]))\n",
    "                    else:\n",
    "                        flatr = rList[r].flatten()\n",
    "                        flatr[0:(NList[r][0]-numFC[r])] = 1\n",
    "                        np.random.shuffle(flatr)\n",
    "                        rList[r] = np.reshape(flatr, (numFC[r],numFC[r+1]))\n",
    "\n",
    "\n",
    "                #For each row place a random 1\n",
    "                for i in range(len(rList[r])):\n",
    "                    #print('r: %s and i: %s' %(r,i))\n",
    "                    row = np.array(rList[r][i])\n",
    "                    zeroElements = np.nonzero(row==0)[0] #Finds all indices with a zero\n",
    "                    idx = np.random.choice(zeroElements) #Picks a random index\n",
    "                    rList[r][i][idx] = 1 #Sets the value at that index to one\n",
    "\n",
    "                if abs(NList[r][2])+numFC[r] < FOMList[r][0]:\n",
    "                    diff = FOMList[r][0] - (abs(NList[r][2])+numFC[r])\n",
    "                    for d in range(diff): #place a random one in the matrix \n",
    "                        z = np.nonzero(rList[r]==0)\n",
    "                        idx = np.random.choice(np.arange(len(z[0])))\n",
    "                        x = z[0][idx]\n",
    "                        y = z[1][idx]\n",
    "                        rList[r][x][y] = 1\n",
    "\n",
    "            if r == 4: \n",
    "                #If the number of nodes in the penultimate layer is less than the nodes in the output layer\n",
    "                if numFC[r] < numFC[r+1]:\n",
    "                    w = FOMList[r][0]\n",
    "                    numExtraCols = numFC[r+1]-numFC[r]\n",
    "                    m = np.eye(numFC[r])\n",
    "                    #If the number of remaining weights is greater that the number of nodes in the output layer\n",
    "                    if w > numFC[r+1]:\n",
    "                        numExtraW = w - numFC[r+1]\n",
    "                        for k in range(numExtraCols):\n",
    "                            col = np.zeros((numFC[r],1))\n",
    "                            z = np.nonzero(col==0)\n",
    "                            idx = np.random.choice(z[0])\n",
    "                            col[idx] = 1\n",
    "                            m = np.append(m, col, axis=1)\n",
    "                        for t in range(numExtraW):\n",
    "                            z = np.nonzero(m==0)\n",
    "                            idx = np.random.choice(np.arange(len(z[0])))\n",
    "                            x = z[0][idx]\n",
    "                            y = z[1][idx]\n",
    "                            m[x][y] = 1\n",
    "                    else:\n",
    "                        for k in range(numExtraCols):\n",
    "                            col = np.zeros((numFC[r],1))\n",
    "                            z = np.nonzero(col==0)\n",
    "                            idx = np.random.choice(z[0])\n",
    "                            col[idx] = 1\n",
    "                            m = np.append(m, col, axis=1)\n",
    "                else: #if numFC[r] >= numFC[r+1]\n",
    "                    w = FOMList[r][0]\n",
    "                    numExtraRows = numFC[r]-numFC[r+1]\n",
    "                    m = np.eye(numFC[r+1])\n",
    "                    #If the number of remaining weights is greater than the number of nodes in the penultimate layer\n",
    "                    if w > numFC[r]:\n",
    "                        numExtraW = w - (numFC[r+1]+numExtraRows)\n",
    "                        for k in range(numExtraRows):\n",
    "                            row = np.zeros((1,numFC[r+1]))\n",
    "                            z = np.nonzero(row==0)\n",
    "                            idx = np.random.choice(z[1])\n",
    "                            row[0][idx] = 1\n",
    "                            m = np.append(m, row, axis=0)\n",
    "                        for t in range(numExtraW):\n",
    "                            z = np.nonzero(m==0)\n",
    "                            idx = np.random.choice(np.arange(len(z[0])))\n",
    "                            x = z[0][idx]\n",
    "                            y = z[1][idx]\n",
    "                            m[x][y] = 1\n",
    "                    else:\n",
    "                        for k in range(numExtraRows):\n",
    "                            row = np.zeros((1,numFC[r+1]))\n",
    "                            z = np.nonzero(row==0)\n",
    "                            idx = np.random.choice(z[1])\n",
    "                            row[0][idx] = 1\n",
    "                            m = np.append(m, row, axis=0)\n",
    "                            \n",
    "                np.random.shuffle(m)\n",
    "                rList[r]=m\n",
    "\n",
    "    '''Create the random bias vectors'''\n",
    "\n",
    "    #Build zeros vector the length of the bias term\n",
    "    r1b = np.zeros((numFC[1]))\n",
    "    r2b = np.zeros((numFC[2]))\n",
    "    r3b = np.zeros((numFC[3]))\n",
    "    r4b = np.zeros((numFC[4]))\n",
    "    r5b = np.zeros((numFC[5]))\n",
    "\n",
    "    #Set the number of live bias terms to 1\n",
    "    r1b[0:FOMList[0][1]] = 1 \n",
    "    r2b[0:FOMList[1][1]] = 1\n",
    "    r3b[0:FOMList[2][1]] = 1 \n",
    "    r4b[0:FOMList[3][1]] = 1 \n",
    "    r5b[0:FOMList[4][1]] = 1 \n",
    "\n",
    "    #Randomly shuffle the bias matrices\n",
    "    np.random.shuffle(r1b)\n",
    "    np.random.shuffle(r2b)\n",
    "    np.random.shuffle(r3b)\n",
    "    np.random.shuffle(r4b)\n",
    "    np.random.shuffle(r5b)\n",
    "\n",
    "    rbList = [r1b, r2b, r3b, r4b, r5b]\n",
    "    \n",
    "    randomNet = []\n",
    "    for i in range(len(rList)):\n",
    "        randomNet.append(np.vstack([rList[i],rbList[i]]))\n",
    "\n",
    "\n",
    "    return randomNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildRandomNet(numFC, FOMList):\n",
    "    '''\n",
    "    Builds randomly connected network with the same number of weights and bias connections as the real network. \n",
    "\n",
    "    Input(s): numFC (Number of remaining nodes with downstream output), FOMList (Number of weights and number of \n",
    "        bias connections in each layer)\n",
    "    Returns: the mask of the random network, as a list of matrices\n",
    "    '''\n",
    "    #random weight matrix = np.array(([1]*num connections between weights)+\n",
    "    #                       [0]*(num possible connections - num connections between weights))\n",
    "    #           numFC[0]-1 because we need to discount bias \n",
    "    r1 = np.array([1] * (FOMList[0][0]) + [0] * (((numFC[0]-1)*(numFC[1]-1))-(FOMList[0][0])))\n",
    "    #random bias matrix = np.array(([1]*num connections between bias and next nodes)+\n",
    "    #                       [0]*(num possible connections - num connections between bias and next nodes))\n",
    "\n",
    "    #There is always a live bias, so number of possible connections between bias and next\n",
    "    #   layer would be numFC[i+1]-1 (to remove bias).\n",
    "    r1b = np.array([1] * FOMList[0][1] + [0] * ((numFC[1]-1)-FOMList[0][1]))\n",
    "\n",
    "    r2 = np.array([1] * (FOMList[1][0]) + [0] * (((numFC[1]-1)*(numFC[2]-1))-(FOMList[1][0])))\n",
    "    r3 = np.array([1] * (FOMList[2][0]) + [0] * (((numFC[2]-1)*(numFC[3]-1))-(FOMList[2][0])))\n",
    "    r4 = np.array([1] * (FOMList[3][0]) + [0] * (((numFC[3]-1)*(numFC[4]-1))-(FOMList[3][0])))\n",
    "    r5 = np.array([1] * (FOMList[4][0]) + [0] * (((numFC[4]-1)*(numFC[5]))-(FOMList[4][0]))) #no bias in last layer\n",
    "\n",
    "    r2b = np.array([1] * FOMList[1][1] + [0] * ((numFC[2]-1)-FOMList[1][1]))\n",
    "    r3b = np.array([1] * FOMList[2][1] + [0] * ((numFC[3]-1)-FOMList[2][1]))\n",
    "    r4b = np.array([1] * FOMList[3][1] + [0] * ((numFC[4]-1)-FOMList[3][1]))\n",
    "    r5b = np.array([1] * FOMList[4][1] + [0] * ((numFC[5])-FOMList[4][1]))\n",
    "        \n",
    "    np.random.shuffle(r1)\n",
    "    np.random.shuffle(r2)\n",
    "    np.random.shuffle(r3)\n",
    "    np.random.shuffle(r4)\n",
    "    np.random.shuffle(r5)\n",
    "\n",
    "    np.random.shuffle(r1b)\n",
    "    np.random.shuffle(r2b)\n",
    "    np.random.shuffle(r3b)\n",
    "    np.random.shuffle(r4b)\n",
    "    np.random.shuffle(r5b)\n",
    "            \n",
    "    randomNet = [np.vstack([np.reshape(r1, (((numFC[0]-1),(numFC[1]-1)))),r1b]),\n",
    "                np.vstack([np.reshape(r2, (((numFC[1]-1),(numFC[2]-1)))),r2b]),\n",
    "                np.vstack([np.reshape(r3, (((numFC[2]-1),(numFC[3]-1)))),r3b]),\n",
    "                np.vstack([np.reshape(r4, (((numFC[3]-1),(numFC[4]-1)))),r4b]),\n",
    "                np.vstack([np.reshape(r5, (((numFC[4]-1),(numFC[5])))),r5b])]\n",
    "\n",
    "    return randomNet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find motifs for random networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomNetMotifs(randomNet):\n",
    "    '''\n",
    "    Finds all of the motifs for the random network.\n",
    "\n",
    "    Input(s): the mask of the random network, as a list of matrices\n",
    "    Returns: rFOM (random first-order motifs), rFOMList (remaining connections in each layer), rSODM (random second-oder \n",
    "        diverging motifs), rSOCM (random second-order converging motifs), rSOChain (random second-order chain motifs), \n",
    "        rTODM (random third-order diverging motifs), rTOCM (random third-order converging motifs), rTOChain (random \n",
    "        third-order chain motifs)\n",
    "    '''\n",
    "\n",
    "    rFOM, rFOMList = fom(randomNet)\n",
    "    rSODM, rnumFC = sodm(randomNet)\n",
    "    rSOCM, rnumFCUS = socm(randomNet)\n",
    "    rSOChain = sochain(randomNet)\n",
    "    rTODM = todm(randomNet)\n",
    "    rTOCM = tocm(randomNet)\n",
    "    rTOChain = tochain(randomNet)\n",
    "    rBIFAN = bifan(randomNet)\n",
    "    rBIPAR = bipar(randomNet)\n",
    "    \n",
    "    return rFOM, rFOMList, rSODM, rSOCM, rSOChain, rTODM, rTOCM, rTOChain, rBIFAN, rBIPAR, rnumFC, rnumFCUS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average random motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildRandomMotifsDF(numFC, FOMList, numRand=1000):\n",
    "    '''\n",
    "    Builds dataframe of numRand number of random network motif counts. \n",
    "    To calculate the z-score, we need to compare the real network to many randomly generated networks. This function \n",
    "        adds all of that information to a dataframe. \n",
    "\n",
    "    Input(s): numFC (Number of remaining nodes with downstream output), FOMList (Number of weights and number of \n",
    "        bias connections in each layer), numRand=1000 (number of random networks we want to generate, default 1000)\n",
    "    Returns: random network motif dataframe\n",
    "    '''\n",
    "    randomNetDF = pd.DataFrame(columns=['rSODM',\n",
    "                                        'rSOCM',\n",
    "                                        'rSOChain',\n",
    "                                        'rTODM',\n",
    "                                        'rTOCM', \n",
    "                                        'rTOChain',\n",
    "                                        'rBIFAN',\n",
    "                                        'rBIPAR',\n",
    "                                        'rnumFC',\n",
    "                                        'rnumFCUS'])\n",
    "\n",
    "    for r in range(numRand):\n",
    "        randomNet = randomPruning(FOMList, numFC)\n",
    "        rFOM, rFOMList, rSODM, rSOCM, rSOChain, rTODM, rTOCM, rTOChain, rBIFAN, rBIPAR, rnumFC, rnumFCUS = randomNetMotifs(randomNet)\n",
    "\n",
    "        rMotifs = [float(rSODM), float(rSOCM), float(rSOChain), float(rTODM), float(rTOCM), float(rTOChain), float(rBIFAN), float(rBIPAR), rnumFC, rnumFCUS]\n",
    "        randomNetDF.loc[len(randomNetDF.index)] = rMotifs\n",
    "\n",
    "    return randomNetDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z-score dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove ghost and dead nodes from the networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sparseMasks_wo_G = rmGhostNodes(sparseMasks)\n",
    "sparseMasks_wo_G_D = rmDeadNodes(sparseMasks_wo_G)\n",
    "\n",
    "count=0\n",
    "for (sparsity, m) in sparseMasks:\n",
    "    FOM, FOMList = fom(m)\n",
    "    SODM, numFC = sodm(m)\n",
    "    SOCM, numFCUS = socm(m)\n",
    "\n",
    "    randNet = randomPruning(FOMList, numFC)\n",
    "\n",
    "    rFOM, rFOMList = fom(randNet)\n",
    "    rSODM, rnumFC = sodm(randNet)\n",
    "    rSOCM, rnumFCUS = socm(randNet)\n",
    "\n",
    "    if FOMList != rFOMList:\n",
    "        print('Network %s' %(count))\n",
    "        print('Real numFC down: %s' %(numFC))\n",
    "        print('Real FOMList: %s' %(FOMList))\n",
    "        print('Random FOMList: %s' %(rFOMList))\n",
    "\n",
    "    if numFC != rnumFC:\n",
    "        print('Network %s' %(count))\n",
    "        print('Real numFC down: %s' %(numFC))\n",
    "        print('Random numFC down: %s' %(rnumFC))\n",
    "\n",
    "    if numFCUS != rnumFCUS:\n",
    "        print('Network %s' %(count))\n",
    "        print('Real numFC up: %s' %(numFCUS))\n",
    "        print('Random numFC up: %s' %(rnumFCUS))\n",
    "\n",
    "    count+=1\n",
    "\n",
    "print('done checking')\n",
    "    \n",
    "'''\n",
    "randNet_wo_G = rmGhostNodes(randNetX)\n",
    "randNet_wo_G_D = rmDeadNodes(randNet_wo_G)\n",
    "\n",
    "for (sparsity, m) in sparseMasks_wo_G_D:\n",
    "    FOM, numW_and_B = fom(m)\n",
    "    SODM, numFC = sodm(m)\n",
    "    SOCM, numFCUS = socm(m)\n",
    "    \n",
    "    randNet = randNet_wo_G_D[1]\n",
    "\n",
    "    rFOM, rFOMList = fom(randNet)\n",
    "    rSODM, rnumFC = sodm(randNet)\n",
    "    rSOCM, rnumFCUS = socm(randNet)\n",
    "\n",
    "    print(numW_and_B)\n",
    "    print(rFOMList)\n",
    "    print(numFC)\n",
    "    print(rnumFC)\n",
    "    print(numFCUS)\n",
    "    print(rnumFCUS)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparseMasks = sparseMasks[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparseMasks_wo_G = rmGhostNodes(sparseMasks, rm=True)\n",
    "sparseMasks_wo_G_D = rmDeadNodes(sparseMasks_wo_G, rm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscoreDF = pd.DataFrame(columns=['Sparsity Index', 'Masks',\n",
    "                                '1-O motifs (real)', \n",
    "                                'S-O diverging motifs (real)', 'S-O converging motifs (real)', \n",
    "                                'S-O chain motifs (real)',  'T-O chain motifs (real)',\n",
    "                                'T-O diverging motifs (real)', 'T-O converging motifs (real)',\n",
    "                                'T-O Bi-Fan motifs (real)', 'T-O Bi-Parallel motifs (real)',\n",
    "                                \n",
    "                                'Avg - S-O diverging motifs (random)', 'Avg - S-O converging motifs (random)', \n",
    "                                'Avg - S-O chain motifs (random)',  'Avg - T-O chain motifs (random)',\n",
    "                                'Avg - T-O diverging motifs (random)', 'Avg - T-O converging motifs (random)',\n",
    "                                'Avg - T-O Bi-Fan motifs', 'Avg - T-O Bi-Parallel motifs',\n",
    "\n",
    "                                 \n",
    "                                'SD - S-O diverging motifs (random)', 'SD - S-O converging motifs (random)', \n",
    "                                'SD - S-O chain motifs (random)',  'SD - T-O chain motifs (random)',\n",
    "                                'SD - T-O diverging motifs (random)', 'SD - T-O converging motifs (random)',\n",
    "                                'SD - T-O Bi-Fan motifs', 'SD - T-O Bi-Parallel motifs',\n",
    "                                \n",
    "                                'Z - S-O diverging motifs', 'Z - S-O converging motifs', \n",
    "                                'Z - S-O chain motifs',  'Z - T-O chain motifs',\n",
    "                                'Z - T-O diverging motifs', 'Z - T-O converging motifs',\n",
    "                                'Z - T-O Bi-Fan motifs', 'Z - T-O Bi-Parallel motifs',\n",
    "\n",
    "                                'Number of nodes in each layer with downstream output',\n",
    "                                'Number of nodes in each layer with upstream input', \n",
    "                                'Number of connections in each layer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (sparsity, m) in sparseMasks_wo_G_D: \n",
    "    FOM, FOMList = fom(m)\n",
    "    SODM, numFC = sodm(m)\n",
    "    SOCM, numFCUS = socm(m)\n",
    "    SOChain = sochain(m)\n",
    "    TODM = todm(m)\n",
    "    TOCM = tocm(m)\n",
    "    TOChain = tochain(m)\n",
    "    BIFAN = bifan(m)\n",
    "    BIPAR = bipar(m)\n",
    "\n",
    "    randomNetDF = buildRandomMotifsDF(numFC, FOMList, numRand=1000)\n",
    "\n",
    "    AvgrSODM = randomNetDF['rSODM'].mean()\n",
    "    AvgrSOCM = randomNetDF['rSOCM'].mean()\n",
    "    AvgrSOChain = randomNetDF['rSOChain'].mean()\n",
    "    AvgrTODM = randomNetDF['rTODM'].mean()\n",
    "    AvgrTOCM = randomNetDF['rTOCM'].mean()\n",
    "    AvgrTOChain = randomNetDF['rTOChain'].mean()\n",
    "    AvgrBIFAN = randomNetDF['rBIFAN'].mean()\n",
    "    AvgrBIPAR = randomNetDF['rBIPAR'].mean()\n",
    "\n",
    "    SDrSODM = randomNetDF['rSODM'].std()\n",
    "    SDrSOCM = randomNetDF['rSOCM'].std()\n",
    "    SDrSOChain = randomNetDF['rSOChain'].std()\n",
    "    SDrTODM = randomNetDF['rTODM'].std()\n",
    "    SDrTOCM = randomNetDF['rTOCM'].std()\n",
    "    SDrTOChain = randomNetDF['rTOChain'].std()\n",
    "    SDrBIFAN = randomNetDF['rBIFAN'].std()\n",
    "    SDrBIPAR = randomNetDF['rBIPAR'].std()\n",
    "\n",
    "    ZSODM = (SODM - AvgrSODM)/SDrSODM\n",
    "    ZSOCM = (SOCM - AvgrSOCM)/SDrSOCM\n",
    "    ZSOChain = (SOChain - AvgrSOChain)/SDrSOChain\n",
    "    ZTODM = (TODM - AvgrTODM)/SDrTODM\n",
    "    ZTOCM = (TOCM - AvgrTOCM)/SDrTOCM\n",
    "    ZTOChain = (TOChain - AvgrTOChain)/SDrTOChain\n",
    "    ZBIFAN = (BIFAN - AvgrBIFAN)/SDrBIFAN\n",
    "    ZBIPAR = (BIPAR - AvgrBIPAR)/SDrBIPAR\n",
    "\n",
    "    zscoreData = [float(sparsity), m, \n",
    "                    float(FOM), \n",
    "                    float(SODM), float(SOCM), float(SOChain),\n",
    "                    float(TODM), float(TOCM), float(TOChain),\n",
    "                    float(BIFAN), float(BIPAR),\n",
    "\n",
    "                    float(AvgrSODM), float(AvgrSOCM), float(AvgrSOChain),\n",
    "                    float(AvgrTODM), float(AvgrTOCM), float(AvgrTOChain),\n",
    "                    float(AvgrBIFAN), float(AvgrBIPAR),\n",
    "\n",
    "                    float(SDrSODM), float(SDrSOCM), float(SDrSOChain),\n",
    "                    float(SDrTODM), float(SDrTOCM), float(SDrTOChain),\n",
    "                    float(SDrBIFAN), float(SDrBIPAR),\n",
    "\n",
    "                    float(ZSODM), float(ZSOCM), float(ZSOChain),\n",
    "                    float(ZTODM), float(ZTOCM), float(ZTOChain),\n",
    "                    float(ZBIFAN), float(ZBIPAR),\n",
    "\n",
    "                    numFC, numFCUS, FOMList]\n",
    "\n",
    "    zscoreDF.loc[len(zscoreDF.index)] = zscoreData\n",
    "\n",
    "zscoreDF.to_csv(os.path.join(zscoreSubdir, 'zscoreDF.csv'))\n",
    "print(zscoreDF.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allsparseMasks_wo_G = rmGhostNodes(allPrunedNets, rm=True, allnets=True)\n",
    "#allsparseMasks_wo_G_D = rmDeadNodes(allsparseMasks_wo_G, rm=True, allNets=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d6a6d34a990875f37da0b077524bbbfd2c78b6a0ee13e7ea4abe1777b8c60f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
